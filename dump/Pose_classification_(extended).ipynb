{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "swiAP0RYqqVM"
   },
   "outputs": [],
   "source": [
    "from pose_classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfE7C9wHel_7"
   },
   "source": [
    "# Step 2: Classification\n",
    "\n",
    "**Important!!** Check that you are using the same classification parameters as while building classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "nYkZJ3a_2MW_"
   },
   "outputs": [],
   "source": [
    "class_name='dumbbell_up'\n",
    "out_video_path = 'test3-out.mov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "A3gGMDE0R2pe"
   },
   "outputs": [],
   "source": [
    "# Open the video.\n",
    "import cv2\n",
    "video_cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Get some video parameters to generate output video with classificaiton.\n",
    "video_n_frames = video_cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "video_fps = video_cap.get(cv2.CAP_PROP_FPS)\n",
    "video_width = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "video_height = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "7t_ACEmTSOhr"
   },
   "outputs": [],
   "source": [
    "# Initilize tracker, classifier and counter.\n",
    "# Do that before every video as all of them have state.\n",
    "from mediapipe.python.solutions import pose as mp_pose\n",
    "\n",
    "\n",
    "# Folder with pose class CSVs. That should be the same folder you using while\n",
    "# building classifier to output CSVs.\n",
    "pose_samples_folder = 'fitness_poses_csvs_out'\n",
    "\n",
    "# Initialize tracker.\n",
    "pose_tracker = mp_pose.Pose(upper_body_only=False)\n",
    "\n",
    "# Initialize embedder.\n",
    "pose_embedder = FullBodyPoseEmbedder()\n",
    "\n",
    "# Initialize classifier.\n",
    "# Ceck that you are using the same parameters as during bootstrapping.\n",
    "pose_classifier = PoseClassifier(\n",
    "    pose_samples_folder=pose_samples_folder,\n",
    "    pose_embedder=pose_embedder,\n",
    "    top_n_by_max_distance=30,\n",
    "    top_n_by_mean_distance=10)\n",
    "\n",
    "# # Uncomment to validate target poses used by classifier and find outliers.\n",
    "# outliers = pose_classifier.find_pose_sample_outliers()\n",
    "# print('Number of pose sample outliers (consider removing them): ', len(outliers))\n",
    "\n",
    "# Initialize EMA smoothing.\n",
    "pose_classification_filter = EMADictSmoothing(\n",
    "    window_size=10,\n",
    "    alpha=0.2)\n",
    "\n",
    "# Initialize counter.\n",
    "repetition_counter = RepetitionCounter(\n",
    "    class_name=class_name,\n",
    "    enter_threshold=6,\n",
    "    exit_threshold=4)\n",
    "\n",
    "# Initialize renderer.\n",
    "pose_classification_visualizer = PoseClassificationVisualizer(\n",
    "    class_name=class_name,\n",
    "    plot_x_max=video_n_frames,\n",
    "    # Graphic looks nicer if it's the same as `top_n_by_mean_distance`.\n",
    "    plot_y_max=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4lXymkneOjgZ",
    "outputId": "cc72fd5e-7acb-4a08-ac7e-d2b156773c83",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no camera\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-4a78efbea435>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# Release MediaPipe resources.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mpose_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Show the last frame of the video.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/mediapipe/python/solution_base.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;34m\"\"\"Closes all the input sources and the graph.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_stream_type_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "# Open output video.\n",
    "out_video = cv2.VideoWriter(out_video_path, cv2.VideoWriter_fourcc(*'mp4v'), video_fps, (video_width, video_height))\n",
    "\n",
    "frame_idx = 0\n",
    "output_frame = None\n",
    "with tqdm.tqdm(total=video_n_frames, position=0, leave=True) as pbar:\n",
    "  while True:\n",
    "    # Get next frame of the video.\n",
    "    success, input_frame = video_cap.read()\n",
    "    if not success:\n",
    "    \n",
    "        print(\"no camera\")\n",
    "        break\n",
    "\n",
    "    # Run pose tracker.\n",
    "    input_frame = cv2.cvtColor(input_frame, cv2.COLOR_BGR2RGB)\n",
    "    result = pose_tracker.process(image=input_frame)\n",
    "    pose_landmarks = result.pose_landmarks\n",
    "\n",
    "    # Draw pose prediction.\n",
    "    output_frame = input_frame.copy()\n",
    "    if pose_landmarks is not None:\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image=output_frame,\n",
    "          landmark_list=pose_landmarks,\n",
    "          connections=mp_pose.POSE_CONNECTIONS)\n",
    "    \n",
    "    if pose_landmarks is not None:\n",
    "      # Get landmarks.\n",
    "      frame_height, frame_width = output_frame.shape[0], output_frame.shape[1]\n",
    "      pose_landmarks = np.array([[lmk.x * frame_width, lmk.y * frame_height, lmk.z * frame_width]\n",
    "                                 for lmk in pose_landmarks.landmark], dtype=np.float32)\n",
    "      assert pose_landmarks.shape == (33, 3), 'Unexpected landmarks shape: {}'.format(pose_landmarks.shape)\n",
    "\n",
    "      # Classify the pose on the current frame.\n",
    "      pose_classification = pose_classifier(pose_landmarks)\n",
    "\n",
    "      # Smooth classification using EMA.\n",
    "      pose_classification_filtered = pose_classification_filter(pose_classification)\n",
    "\n",
    "      # Count repetitions.\n",
    "      repetitions_count = repetition_counter(pose_classification_filtered)\n",
    "    else:\n",
    "      # No pose => no classification on current frame.\n",
    "      pose_classification = None\n",
    "\n",
    "      # Still add empty classification to the filter to maintaing correct\n",
    "      # smoothing for future frames.\n",
    "      pose_classification_filtered = pose_classification_filter(dict())\n",
    "      pose_classification_filtered = None\n",
    "\n",
    "      # Don't update the counter presuming that person is 'frozen'. Just\n",
    "      # take the latest repetitions count.\n",
    "      repetitions_count = repetition_counter.n_repeats\n",
    "\n",
    "    # Draw classification plot and repetition counter.\n",
    "    output_frame = pose_classification_visualizer(\n",
    "        frame=output_frame,\n",
    "        pose_classification=pose_classification,\n",
    "        pose_classification_filtered=pose_classification_filtered,\n",
    "        repetitions_count=repetitions_count)\n",
    "\n",
    "    # Save the output frame.\n",
    "    #out_video.write(cv2.cvtColor(np.array(output_frame), cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    # Show intermediate frames of the video to track progress.\n",
    "    if frame_idx % 50 == 0:\n",
    "      show_image(output_frame)\n",
    "\n",
    "    frame_idx += 1\n",
    "    pbar.update()\n",
    "\n",
    "# Close output video.\n",
    "out_video.release()\n",
    "\n",
    "# Release MediaPipe resources.\n",
    "pose_tracker.close()\n",
    "\n",
    "# Show the last frame of the video.\n",
    "if output_frame is not None:\n",
    "  show_image(output_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "xJFftPiQE56E",
    "outputId": "49780a52-c36b-46e5-e676-04565308912a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-fbde7643051d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Download generated video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_video_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "# Download generated video\n",
    "files.download(out_video_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pose classification (extended).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
